version: "3.7"

networks:
  test-bench.gw.network:
    driver: bridge
  test-bench.db.network:
    driver: bridge
  test-bench.srv.network:
    driver: bridge
  test-bench.elk.network:
    driver: bridge
  test-bench.metrics.network:
    driver: bridge

volumes:
  prometheus_data: {}
  grafana_data: {}
  es01: {}
  postgresql: {}
  postgresql_data: {}

services:

  redis:
    image: redis
    container_name: r01
    networks:
      - test-bench.db.network

  postgres:
    build: ../postgres
    container_name: pg01
    networks:
      - test-bench.db.network
    restart: always
    ports:
      - 5432:5432
    environment:
      - POSTGRES_USER=admin
      - POSTGRES_PASSWORD=password
    volumes:
      - postgresql:/var/lib/postgresql
      # This needs explicit mapping due to https://github.com/docker-library/postgres/blob/4e48e3228a30763913ece952c611e5e9b95c8759/Dockerfile.template#L52
      - postgresql_data:/var/lib/postgresql/data

  user:
    build: ../user
    container_name: uss01
    links:
      - "postgres"
    depends_on:
      - postgres
    restart: always
    networks:
      - test-bench.srv.network
      - test-bench.db.network
    ports:
      - 8001:8080
#    logging:
#      driver: "fluentd"
#      options:
#        fluentd-address: localhost:24224
#        fluentd-async-connect: 'true'
#        fluentd-retry-wait: '1s'
#        fluentd-max-retries: '30'
#        tag: service.logback.user

  endpoint:
    build: ../endpoint
    container_name: eps01
    links:
      - "user"
    depends_on:
      - user
    restart: always
    environment:
      SECRET: password
      BACKEND_URL: http://user:8080/
    networks:
      - test-bench.srv.network
    logging:
      driver: "fluentd"
      options:
        fluentd-address: localhost:24224
        fluentd-async-connect: 'true'
        fluentd-retry-wait: '1s'
        fluentd-max-retries: '30'
        tag: service.logback.endpoint

  prometheus:
    image: prom/prometheus
    container_name: p01
    volumes:
      - ./prometheus/:/etc/prometheus/
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
    networks:
      - test-bench.srv.network
      - test-bench.metrics.network
    ports:
      - 8101:9090
    restart: always

  grafana:
    image: grafana/grafana
    container_name: gr01
    links:
      - "prometheus"
    depends_on:
      - prometheus
    networks:
      - test-bench.metrics.network
    ports:
      - 8102:3000
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/provisioning/:/etc/grafana/provisioning/
    env_file:
      - ./grafana/config.monitoring
    restart: always

  cadvisor:
    image: google/cadvisor
    container_name: cad01
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:rw
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
    ports:
      - 8103:8080
    networks:
      - test-bench.srv.network
      - test-bench.metrics.network
    restart: always

  alertmanager:
    image: prom/alertmanager
    container_name: am01
    ports:
      - 8104:9093
    volumes:
      - ./alertmanager/:/etc/alertmanager/
    networks:
      - test-bench.metrics.network
    restart: always
    command:
      - '--config.file=/etc/alertmanager/config.yml'
      - '--storage.path=/alertmanager'

  node-exporter:
    image: prom/node-exporter
    container_name: ne01
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.sysfs=/host/sys'
      - --collector.filesystem.ignored-mount-points
      - "^/(sys|proc|dev|host|etc|rootfs/var/lib/docker/containers|rootfs/var/lib/docker/overlay2|rootfs/run/docker/netns|rootfs/var/lib/docker/aufs)($$|/)"
    ports:
      - 8105:9100
    networks:
      - test-bench.srv.network
      - test-bench.metrics.network
    restart: always

  elasticsearch:
    image: elasticsearch:7.6.0
    expose:
      - 9200
    networks:
      - test-bench.elk.network
    container_name: es01
    environment:
      - node.name=es01
      - cluster.name=es-docker-cluster
      - cluster.initial_master_nodes=es01
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - es01:/usr/share/elasticsearch/data
#    logging:
#      driver: fluentd
#      options:
#        fluentd-address: localhost:24224
#        fluentd-async-connect: 'true'
#        fluentd-retry-wait: '1s'
#        fluentd-max-retries: '30'
#        tag: service.efk.elasticsearch

  kibana:
    image: kibana:7.6.0
    container_name: kb01
    links:
      - "elasticsearch"
    depends_on:
      - elasticsearch
    networks:
      - test-bench.elk.network
    ports:
      - "8106:5601"
#    logging:
#      driver: fluentd
#      options:
#        fluentd-address: localhost:24224
#        fluentd-async-connect: 'true'
#        fluentd-retry-wait: '1s'
#        fluentd-max-retries: '30'
#        tag: service.efk.kibana

  fluentd:
    build: ../fluentd
    container_name: fd01
    links:
      - "elasticsearch"
    depends_on:
      - elasticsearch
    networks:
      - test-bench.srv.network
      - test-bench.elk.network
    volumes:
      - ./fluentd/:/fluentd/etc/
    ports:
      - "24224:24224"
      - "24224:24224/udp"
    logging:
      driver: "json-file"
      options:
        max-size: "1G"
        max-file: "2"


  # Switch storage type to Elasticsearch
  zipkin:
    image: openzipkin/zipkin
    container_name: zp1
    links:
      - "elasticsearch"
    depends_on:
      - elasticsearch
    networks:
      - test-bench.elk.network
      - test-bench.srv.network
    environment:
      - STORAGE_TYPE=elasticsearch
      # Point the zipkin at the storage backend
      - ES_HOSTS=elasticsearch:9200
      # Uncomment to see requests to and from elasticsearch
      # - ES_HTTP_LOGGING=BODY
      # Uncomment to enable scribe
      # - SCRIBE_ENABLED=true
      # Uncomment to enable self-tracing
      # - SELF_TRACING_ENABLED=true
      # Uncomment to enable debug logging
      # - JAVA_OPTS=-Dlogging.level.zipkin2=DEBUG
    ports:
      # Port used for the Zipkin UI and HTTP Api
      - "8107:9411"
      # Uncomment if you set SCRIBE_ENABLED=true
      # - 9410:9410

  # Adds a cron to process spans since midnight every hour, and all spans each day
  # This data is served by http://192.168.99.100:8080/dependency
  #
  # For more details, see https://github.com/openzipkin/docker-zipkin-dependencies
  zipkin-dependencies:
    image: openzipkin/zipkin-dependencies
    container_name: zpd1
    links:
      - "elasticsearch"
    depends_on:
      - elasticsearch
    networks:
      - test-bench.elk.network
    #entrypoint: crond -f
    environment:
      - STORAGE_TYPE=elasticsearch
      - ES_HOSTS=elasticsearch
      # Uncomment to see dependency processing logs
      - ZIPKIN_LOG_LEVEL=DEBUG
      # Uncomment to adjust memory used by the dependencies job
      # - JAVA_OPTS=-verbose:gc -Xms1G -Xmx1G

#https://gist.github.com/jensens/00f329c292fcb68861ec53abc453c5c7
  sentry:
    image: sentry
    container_name: sn1
    links:
      - redis
      - postgres
    depends_on:
      - redis
      - postgres
    ports:
      - 8108:9000
    restart: always
    networks:
      - test-bench.srv.network
      - test-bench.db.network
    environment:
      SENTRY_SECRET_KEY: 'test-bench project sentry secret key 1234557890'
      SENTRY_POSTGRES_HOST: postgres
      SENTRY_DB_NAME: sentry
      SENTRY_DB_USER: admin
      SENTRY_DB_PASSWORD: password
      SENTRY_REDIS_HOST: redis
#      SENTRY_SERVER_EMAIL:
#      SENTRY_EMAIL_HOST:
#      SENTRY_EMAIL_PORT:
#      SENTRY_EMAIL_USER:
#      SENTRY_EMAIL_PASSWORD:
#    command: >
#      sh -c "sentry upgrade"

  sentry_cron:
    image: sentry:latest
    container_name: snc1
    links:
      - redis
      - postgres
    depends_on:
      - redis
      - postgres
    command: "sentry run cron"
    networks:
      - test-bench.srv.network
      - test-bench.db.network
    environment:
      SENTRY_SECRET_KEY: 'test-bench project sentry secret key 1234557890'
      SENTRY_POSTGRES_HOST: postgres
      SENTRY_DB_USER: sentry
      SENTRY_DB_NAME: sentry
      SENTRY_DB_PASSWORD: sentry
      SENTRY_REDIS_HOST: redis

  sentry_worker:
    image: sentry:latest
    container_name: snw1
    links:
      - redis
      - postgres
    depends_on:
      - redis
      - postgres
    command: "sentry run worker"
    networks:
      - test-bench.srv.network
      - test-bench.db.network
    environment:
      SENTRY_SECRET_KEY: 'test-bench project sentry secret key 1234557890'
      SENTRY_POSTGRES_HOST: postgres
      SENTRY_DB_USER: sentry
      SENTRY_DB_NAME: sentry
      SENTRY_DB_PASSWORD: sentry
      SENTRY_REDIS_HOST: redis

  kong:
    build: ../kong
    container_name: kg1
    links:
      - postgres
    depends_on:
      - postgres
    restart: always
    networks:
      - test-bench.gw.network
      - test-bench.srv.network
      - test-bench.db.network
    ports:
      - "8200:8000" # Listener
      - "8201:8001" # Admin API
#      - "8443:8443" # Listener  (SSL)
#      - "8444:8444" # Admin API (SSL)
    environment:
      KONG_DATABASE:         postgres
      KONG_PG_HOST:          postgres
      KONG_PG_PORT:          5432
      KONG_PG_DATABASE:      kong
      KONG_PG_USER:          kong
      KONG_PG_PASSWORD:      kong

      KONG_PROXY_ACCESS_LOG: /dev/stdout
      KONG_ADMIN_ACCESS_LOG: /dev/stdout
      KONG_PROXY_ERROR_LOG:  /dev/stderr
      KONG_ADMIN_ERROR_LOG:  /dev/stderr
      KONG_PROXY_LISTEN:     0.0.0.0:8000, 0.0.0.0:8443 ssl
      KONG_ADMIN_LISTEN:     0.0.0.0:8001, 0.0.0.0:8444 ssl
      KONG_PLUGINS:          oidc,zipkin,prometheus

#      KONG_DECLARATIVE_CONFIG: /usr/local/kong/declarative/kong.yml
#      KONG_LUA_PACKAGE_PATH: /usr/local/kong/declarative/?.lua;;
#    volumes:
#      - ./kong/:/usr/local/kong/declarative
#    command: 'kong migrations up'

# curl -s -X POST http://localhost:8201/services -d name=endpoint -d url=http://endpoint
# curl -s -X POST http://localhost:8201/routes -d service.id=${service_id} -d paths[]=/
# curl -s -X POST http://localhost:8201/routes -d service.id=6144a512-334e-4486-849c-86048fa1a81c -d paths[]=/
# curl -s -X POST http://localhost:8201/plugins -d name=oidc -d config.client_id=kong -d config.client_secret=${CLIENT_SECRET} -d config.discovery=http://${HOST_IP}:8180/auth/realms/master/.well-known/openid-configuration
# curl -s -X POST http://localhost:8201/plugins -d name=oidc -d config.client_id=kong -d config.client_secret=8b691465-4c8d-4215-9469-b971dbb10904 -d config.discovery=http://192.168.1.70:8180/auth/realms/master/.well-known/openid-configuration
# curl -s -X POST http://localhost:8201/plugins -d name=zipkin -d "config.http_endpoint=http://zipkin:9411/api/v2/spans" -d "config.sample_ratio=1"


# client secret: 8b691465-4c8d-4215-9469-b971dbb10904
  keycloak:
    image: quay.io/keycloak/keycloak:latest
    container_name: kc1
    links:
      - postgres
    depends_on:
      - postgres
    networks:
      - test-bench.gw.network
      - test-bench.srv.network
      - test-bench.db.network
    ports:
      - "8180:8080"
    environment:
      DB_VENDOR:   POSTGRES
      DB_ADDR:     postgres
      DB_PORT:     5432
      DB_DATABASE: keycloak
      DB_USER:     keycloak
      DB_PASSWORD: keycloak
      DB_SCHEMA:   public
      KEYCLOAK_USER:     admin
      KEYCLOAK_PASSWORD: admin