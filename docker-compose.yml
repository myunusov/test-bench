version: "3.7"

networks:
  test-bench.gw.network:
    driver: bridge
  test-bench.db.network:
    driver: bridge
  test-bench.srv.network:
    driver: bridge
  test-bench.elk.network:
    driver: bridge
  test-bench.metrics.network:
    driver: bridge

volumes:
  prometheus_data: {}
  grafana_data: {}
  es01: {}
  postgresql: {}
  postgresql_data: {}

services:

  redis:
    image: redis
    networks:
      - test-bench.db.network

  postgres:
    build: ../postgres
    networks:
      - test-bench.db.network
    restart: always
    ports:
      - 5432:5432
    environment:
      - POSTGRES_USER=admin
      - POSTGRES_PASSWORD=password
    volumes:
      - postgresql:/var/lib/postgresql
      # This needs explicit mapping due to https://github.com/docker-library/postgres/blob/4e48e3228a30763913ece952c611e5e9b95c8759/Dockerfile.template#L52
      - postgresql_data:/var/lib/postgresql/data

  user:
    build: ../user
    restart: always
    networks:
      - test-bench.srv.network
      - test-bench.db.network
    links:
      - "postgres"
    logging:
      driver: "fluentd"
      options:
        fluentd-address: localhost:24224
        fluentd-async-connect: 'true'
        fluentd-retry-wait: '1s'
        fluentd-max-retries: '30'
        tag: service.logback.user

  gateway:
    build: ../gateway
    restart: always
    environment:
      SECRET: password
      BACKEND_URL: http://user:8080/
    networks:
      - test-bench.srv.network
    ports:
      - 8081:8080
    logging:
      driver: "fluentd"
      options:
        fluentd-address: localhost:24224
        fluentd-async-connect: 'true'
        fluentd-retry-wait: '1s'
        fluentd-max-retries: '30'
        tag: service.logback.gateway


  prometheus:
    image: prom/prometheus
    volumes:
      - ./prometheus/:/etc/prometheus/
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
    networks:
      - test-bench.srv.network
      - test-bench.metrics.network
    ports:
      - 8101:9090
    restart: always

  grafana:
    image: grafana/grafana
    depends_on:
      - prometheus
    networks:
      - test-bench.metrics.network
    ports:
      - 8102:3000
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/provisioning/:/etc/grafana/provisioning/
    env_file:
      - ./grafana/config.monitoring
    restart: always

  cadvisor:
    image: google/cadvisor
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:rw
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
    ports:
      - 8103:8080
    networks:
      - test-bench.srv.network
      - test-bench.metrics.network
    restart: always

  alertmanager:
    image: prom/alertmanager
    ports:
      - 8104:9093
    volumes:
      - ./alertmanager/:/etc/alertmanager/
    networks:
      - test-bench.metrics.network
    restart: always
    command:
      - '--config.file=/etc/alertmanager/config.yml'
      - '--storage.path=/alertmanager'

  node-exporter:
    image: prom/node-exporter
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.sysfs=/host/sys'
      - --collector.filesystem.ignored-mount-points
      - "^/(sys|proc|dev|host|etc|rootfs/var/lib/docker/containers|rootfs/var/lib/docker/overlay2|rootfs/run/docker/netns|rootfs/var/lib/docker/aufs)($$|/)"
    ports:
      - 8105:9100
    networks:
      - test-bench.srv.network
      - test-bench.metrics.network
    restart: always


  elasticsearch:
    image: elasticsearch:7.6.0
    expose:
      - 9200
    networks:
      - test-bench.elk.network
    container_name: es01
    environment:
      - node.name=es01
      - cluster.name=es-docker-cluster
      - cluster.initial_master_nodes=es01
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - es01:/usr/share/elasticsearch/data
    logging:
      driver: fluentd
      options:
        fluentd-address: localhost:24224
        fluentd-async-connect: 'true'
        fluentd-retry-wait: '1s'
        fluentd-max-retries: '30'
        tag: service.efk.elasticsearch

  kibana:
    image: kibana:7.6.0
    networks:
      - test-bench.elk.network
    links:
      - "elasticsearch"
    ports:
      - "8106:5601"
    logging:
      driver: fluentd
      options:
        fluentd-address: localhost:24224
        fluentd-async-connect: 'true'
        fluentd-retry-wait: '1s'
        fluentd-max-retries: '30'
        tag: service.efk.kibana

  fluentd:
    build: ../fluentd
    links:
      - "elasticsearch"
    networks:
      - test-bench.srv.network
      - test-bench.elk.network
    volumes:
      - ./fluentd/:/fluentd/etc/
    ports:
      - "24224:24224"
      - "24224:24224/udp"
    logging:
      driver: "json-file"
      options:
        max-size: "1G"
        max-file: "2"


  # Switch storage type to Elasticsearch
  zipkin:
    image: openzipkin/zipkin
    networks:
      - test-bench.elk.network
      - test-bench.srv.network
    container_name: zipkin
    environment:
      - STORAGE_TYPE=elasticsearch
      # Point the zipkin at the storage backend
      - ES_HOSTS=elasticsearch:9200
      # Uncomment to see requests to and from elasticsearch
      # - ES_HTTP_LOGGING=BODY
      # Uncomment to enable scribe
      # - SCRIBE_ENABLED=true
      # Uncomment to enable self-tracing
      # - SELF_TRACING_ENABLED=true
      # Uncomment to enable debug logging
      # - JAVA_OPTS=-Dlogging.level.zipkin2=DEBUG
    links:
      - "elasticsearch"
    ports:
      # Port used for the Zipkin UI and HTTP Api
      - "8107:9411"
      # Uncomment if you set SCRIBE_ENABLED=true
      # - 9410:9410

  # Adds a cron to process spans since midnight every hour, and all spans each day
  # This data is served by http://192.168.99.100:8080/dependency
  #
  # For more details, see https://github.com/openzipkin/docker-zipkin-dependencies
  dependencies:
    image: openzipkin/zipkin-dependencies
    networks:
      - test-bench.elk.network
    container_name: dependencies
    entrypoint: crond -f
    environment:
      - STORAGE_TYPE=elasticsearch
      - ES_HOSTS=elasticsearch
      # Uncomment to see dependency processing logs
      - ZIPKIN_LOG_LEVEL=DEBUG
      # Uncomment to adjust memory used by the dependencies job
      # - JAVA_OPTS=-verbose:gc -Xms1G -Xmx1G

#https://gist.github.com/jensens/00f329c292fcb68861ec53abc453c5c7
  sentry:
    image: sentry
    links:
      - redis
      - postgres
    ports:
      - 9000:9000
    restart: always
    networks:
      - test-bench.srv.network
      - test-bench.db.network
    environment:
      SENTRY_SECRET_KEY: 'test-bench project sentry secret key 1234557890'
      SENTRY_POSTGRES_HOST: postgres
      SENTRY_DB_USER: sentry
      SENTRY_DB_NAME: sentry
      SENTRY_DB_PASSWORD: sentry
      SENTRY_REDIS_HOST: redis
#    command: sentry upgrade
#    admin: password

  sentry_cron:
    image: sentry:latest
    depends_on:
      - redis
      - postgres
    command: "sentry run cron"
    networks:
      - test-bench.srv.network
      - test-bench.db.network
    environment:
      SENTRY_SECRET_KEY: 'test-bench project sentry secret key 1234557890'
      SENTRY_POSTGRES_HOST: postgres
      SENTRY_DB_USER: sentry
      SENTRY_DB_NAME: sentry
      SENTRY_DB_PASSWORD: sentry
      SENTRY_REDIS_HOST: redis

  sentry_worker:
    image: sentry:latest
    depends_on:
      - redis
      - postgres
    command: "sentry run worker"
    networks:
      - test-bench.srv.network
      - test-bench.db.network
    environment:
      SENTRY_SECRET_KEY: 'test-bench project sentry secret key 1234557890'
      SENTRY_POSTGRES_HOST: postgres
      SENTRY_DB_USER: sentry
      SENTRY_DB_NAME: sentry
      SENTRY_DB_PASSWORD: sentry
      SENTRY_REDIS_HOST: redis

  kong:
    build: ../kong
    restart: always
    networks:
      - test-bench.gw.network
      - test-bench.srv.network
      - test-bench.db.network
    depends_on:
      - postgres
    ports:
      - "8200:8000" # Listener
      - "8201:8001" # Admin API
#      - "8443:8443" # Listener  (SSL)
#      - "8444:8444" # Admin API (SSL)
    environment:
      KONG_DATABASE:         postgres
      KONG_PG_HOST:          postgres
      KONG_PG_PORT:          5432
      KONG_PG_DATABASE:      kong
      KONG_PG_USER:          kong
      KONG_PG_PASSWORD:      kong

      KONG_PROXY_ACCESS_LOG: /dev/stdout
      KONG_ADMIN_ACCESS_LOG: /dev/stdout
      KONG_PROXY_ERROR_LOG:  /dev/stderr
      KONG_ADMIN_ERROR_LOG:  /dev/stderr
      KONG_PROXY_LISTEN:     0.0.0.0:8000, 0.0.0.0:8443 ssl
      KONG_ADMIN_LISTEN:     0.0.0.0:8001, 0.0.0.0:8444 ssl
      KONG_PLUGINS:          oidc,zipkin

#      KONG_DECLARATIVE_CONFIG: /usr/local/kong/declarative/kong.yml
      KONG_LUA_PACKAGE_PATH: /usr/local/kong/declarative/?.lua;;
#    volumes:
#      - ./kong/:/usr/local/kong/declarative
#    command: 'kong migrations up'

  keycloak:
    image: quay.io/keycloak/keycloak:latest
    depends_on:
      - postgres
    networks:
      - test-bench.gw.network
      - test-bench.srv.network
      - test-bench.db.network
    ports:
      - "8180:8080"
    environment:
      DB_VENDOR:   POSTGRES
      DB_ADDR:     postgres
      DB_PORT:     5432
      DB_DATABASE: keycloak
      DB_USER:     keycloak
      DB_PASSWORD: keycloak
      DB_SCHEMA:   public
      KEYCLOAK_USER:     admin
      KEYCLOAK_PASSWORD: admin